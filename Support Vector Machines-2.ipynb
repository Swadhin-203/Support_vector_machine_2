{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e70cb9d-27f0-46db-8f7a-906b82e6b29b",
   "metadata": {},
   "source": [
    "# Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "Polynomial functions and kernel functions are related in the context of machine learning algorithms, particularly in support vector machines (SVMs) and kernelized methods.\r\n",
    "\r\n",
    "In machine learning, the kernel trick is a technique that allows algorithms to operate in a higher-dimensional space without explicitly computing the transformed feature vectors. This is often used in the context of SVMs to find nonlinear decision boundaries.\r\n",
    "\r\n",
    "Polynomial functions can be used as kernel functions in the kernelized SVM. The polynomial kernel is defined as \\(K(x, y) = (x \\cdot y + c)^d\\), where \\(d\\) is the degree of the polynomial, and \\(c\\) is a constant. This kernel allows the SVM to learn nonlinear decision boundaries by implicitly mapping the input data into a higher-dimensional space defined by polynomial features.\r\n",
    "\r\n",
    "The polynomial kernel is just one example of a kernel function. Other common kernel functions include the linear kernel (\\(K(x, y) = x \\cdot y\\)), radial basis function (RBF) kernel, and sigmoid kernel. Each kernel function defines a different way to measure the similarity or distance between data points in the feature space.\r\n",
    "\r\n",
    "In summary, polynomial functions can be used as kernel functions in machine learning algorithms like SVMs to enable them to handle nonlinear relationships in the data by implicitly mapping the data into a higher-dimensional space. The choice of kernel function, including polynomial kernels, depends on the characteristics of the data and the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421aaf7a-b2b0-40e4-8172-d49e0f6756d3",
   "metadata": {},
   "source": [
    "# Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\r\n",
    "Implementing an SVM with a polynomial kernel in Python using Scikit-learn involves using the SVC (Support Vector Classification) class with the kernel='poly' parameter.\n",
    "In this below example:\r\n",
    "\r\n",
    "We use the Iris dataset, but you can replace it with your own dataset.\r\n",
    "The data is split into training and testing sets.\r\n",
    "Standardization is applied to scale the features.\r\n",
    "An SVM with a polynomial kernel is created using SVC with kernel='poly', and you can specify the degree of the polynomial using the degree parameter.\r\n",
    "The SVM is trained on the training data.\r\n",
    "Predictions are made on the test set, and accuracy is evaluated.\r\n",
    "Adjust the parameters like degree and C based on your specific problem and dataset. The C parameter controls the regularization strength, and the degree parameter determines the degree of the polynomial kernel. Experiment with different values to find the best hyperparameters for your problem.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf74768-49ec-40d0-a48c-dfcba0ef0a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.67%\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load a sample dataset, such as the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create an SVM with a polynomial kernel\n",
    "degree = 3  # Degree of the polynomial kernel\n",
    "C = 1.0  # Regularization parameter\n",
    "svm_poly = SVC(kernel='poly', degree=degree, C=C)\n",
    "\n",
    "# Train the SVM\n",
    "svm_poly.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_poly.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287e530e-b167-42a3-9e1c-30e924d7caf9",
   "metadata": {},
   "source": [
    "# Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "In Support Vector Regression (SVR), epsilon (\\(\\varepsilon\\)) is a crucial parameter associated with the margin of the support vector tube. The SVR algorithm aims to fit the data within a margin around the predicted values, and epsilon controls the width of this margin. Specifically, there are two parameters in SVR related to epsilon:\n",
    "\n",
    "1. **Epsilon-insensitive loss (epsilon):** This is the parameter that determines the width of the epsilon-tube around the predicted values. Any prediction within this tube is considered accurate and does not contribute to the loss function. The larger the epsilon, the wider the tube, and the more tolerance there is for errors.\n",
    "\n",
    "2. **Tolerance (tol):** This is another parameter that affects the number of support vectors indirectly. It is used to set the tolerance for stopping criteria. A smaller tolerance value may lead to a larger number of support vectors.\n",
    "\n",
    "Now, to address your question: as you increase the value of epsilon in SVR, it typically results in a wider tube, allowing more data points to fall within the acceptable error range. Consequently, the SVR algorithm may become less sensitive to small deviations from the predicted values, and more data points might be considered as within the margin.\n",
    "\n",
    "However, it's important to note that the relationship between epsilon and the number of support vectors can be influenced by other factors, such as the specific characteristics of the data, the choice of kernel function, and the overall complexity of the model. In some cases, increasing epsilon might lead to an increase in the number of support vectors, while in other cases, it might reduce the number.\n",
    "\n",
    "In practice, when tuning the parameters of an SVR model, including epsilon, it's common to perform cross-validation to find the optimal set of parameters for the specific problem at hand. This allows you to assess the model's performance under different parameter configurations and choose the combination that results in the best generalization to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee560a64-0b5e-46cf-b6de-55b78db35122",
   "metadata": {},
   "source": [
    "# Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n",
    "Support Vector Regression (SVR) is a machine learning algorithm that uses support vector machines for regression tasks. The performance of SVR is influenced by several key parameters: the choice of kernel function, C parameter, epsilon parameter, and gamma parameter. Let's discuss each parameter and its impact on SVR performance:\n",
    "\n",
    "1. **Kernel Function:**\n",
    "   - **Purpose:** The kernel function determines the type of decision boundary that the SVR model will learn. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "   - **Example:**\n",
    "     - Use a linear kernel (`kernel='linear'`) when the relationship between the input features and the target variable is expected to be linear.\n",
    "     - Use an RBF kernel (`kernel='rbf'`) for non-linear relationships.\n",
    "\n",
    "2. **C Parameter:**\n",
    "   - **Purpose:** The C parameter controls the trade-off between having a smooth decision boundary and fitting the training data accurately. A smaller C encourages a smoother boundary, while a larger C allows the model to fit the training data more closely.\n",
    "   - **Example:**\n",
    "     - Increase C when you want the model to closely fit the training data, but be cautious about overfitting.\n",
    "     - Decrease C when you want a smoother decision boundary and are willing to sacrifice some training data fitting.\n",
    "\n",
    "3. **Epsilon Parameter (epsilon-insensitive loss):**\n",
    "   - **Purpose:** Epsilon (\\(\\varepsilon\\)) defines the tube around the regression line within which no penalty is associated with errors. It controls the width of this tube.\n",
    "   - **Example:**\n",
    "     - Increase epsilon if you want the model to be more tolerant of errors, allowing a wider margin for predictions.\n",
    "     - Decrease epsilon if you want to penalize predictions that deviate even slightly from the true values.\n",
    "\n",
    "4. **Gamma Parameter:**\n",
    "   - **Purpose:** The gamma parameter defines how far the influence of a single training example reaches. Low values mean a broader influence, and high values mean a more localized influence.\n",
    "   - **Example:**\n",
    "     - Increase gamma for a more localized influence, which can lead to a more complex decision boundary.\n",
    "     - Decrease gamma for a broader influence, useful when the data has a global pattern.\n",
    "\n",
    "It's crucial to note that the optimal values for these parameters depend on the specific characteristics of your dataset. Cross-validation is often used to find the best combination of hyperparameters for SVR. Here are some general guidelines:\n",
    "\n",
    "- Start with a wide range of values for each parameter.\n",
    "- Use cross-validation to evaluate the model's performance for different combinations of parameters.\n",
    "- Fine-tune the parameters based on the cross-validation results.\n",
    "\n",
    "The choice of parameters in SVR is a balance between model complexity and generalization to new, unseen data. Regularization parameters like C and epsilon should be chosen carefully to prevent overfitting and ensure good performance on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b86168c-8af3-4cb1-bc61-66865a29a8d8",
   "metadata": {},
   "source": [
    "# Q5. Assignment:\n",
    "### Import the necessary libraries and load the dataseg\n",
    "###  Split the dataset into training and testing setZ\n",
    "###  Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "###  Create an instance of the SVC classifier and train it on the training datW\n",
    "### hse the trained classifier to predict the labels of the testing datW\n",
    "###  Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "### precision, recall, F1-scoreK\n",
    "###  Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "### improve its performanc_\n",
    "###  Train the tuned classifier on the entire dataseg\n",
    "### Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "103e835c-a987-4157-b997-aac564f57c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "Best Hyperparameters: {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tuned_svc_classifier.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data - Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Additional evaluation metrics (precision, recall, F1-score)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print('Classification Report:\\n', report)\n",
    "\n",
    "# Tune hyperparameters using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 0.01, 0.001], 'kernel': ['linear', 'rbf', 'poly']}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print('Best Hyperparameters:', best_params)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "tuned_svc = SVC(**best_params)\n",
    "tuned_svc.fit(X_train_scaled, y_train)  # Use X_train_scaled for training\n",
    "\n",
    "# Save the trained classifier to a file for future use\n",
    "joblib.dump(tuned_svc, 'tuned_svc_classifier.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e225c06-3bf4-42f8-946b-801bef0494b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07446b16-d77f-444e-bc96-33637d93a555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
